{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNngJHDj07DqKeyyp3P71ky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkmariya/Scaler01/blob/master/hpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning Example in ML\n",
        "\n",
        "\n",
        "Hyperparameter tuning in machine learning (ML) and deep learning (DL) refers to the process of optimizing the configuration settings used to control the learning process and structure of a machine learning or deep learning model. These hyperparameters are not learned from the data; instead, they are set prior to the training process and have a significant effect on the performance of the model.\n",
        "\n",
        "Hyperparameters can include:\n",
        "\n",
        "1.   Learning rate: The step size at which the model learns from the data.\n",
        "2.   Number of epochs: The number of times the learning algorithm will work through the entire training dataset.\n",
        "3.   Batch size: The number of training examples utilized in one iteration.\n",
        "4.   Network architecture specifics: Such as the number of layers and the number of units per layer in neural networks.\n",
        "5.   Regularization parameters: Such as weight decay coefficients or dropout rates.\n",
        "6.   Optimization algorithms: Such as Adam, SGD, RMSprop, etc.\n",
        "\n",
        "The goal of hyperparameter tuning is to find the set of hyperparameters that yields the best performance on a given task. This is typically measured by a performance metric such as accuracy, F1 score, or mean squared error, depending on the problem at hand.\n",
        "\n",
        "The methods used for hyperparameter tuning can generally be applied both in ML and DL, and they include:\n",
        "\n",
        "**Grid Search**: An exhaustive search that systematically goes through every combination of hyperparameters in a predefined grid. It's computationally expensive, especially with a large number of hyperparameters.\n",
        "\n",
        "**Random Search**: Instead of trying out every possible combination, random search selects random combinations of hyperparameters to try. This method can sometimes find a good set of hyperparameters more quickly than grid search.\n",
        "\n",
        "**Bayesian Optimization**: This approach models the performance function of the hyperparameters using a Gaussian process or a similar probabilistic model and then selects hyperparameters to try based on how likely they are to yield a performance improvement.\n",
        "\n",
        "**Gradient-based Optimization**: For some kinds of hyperparameters, it’s possible to compute the gradient with respect to the hyperparameters and optimize them directly. This is more common in deep learning with specific types of hyperparameters.\n",
        "\n",
        "**Evolutionary Algorithms**: These algorithms simulate the process of natural selection to select, mutate, and recombine hyperparameters in search of the most effective configuration.\n",
        "\n",
        "**Hyperband**: A bandit-based approach that dynamically allocates resources to hyperparameter configurations and rapidly eliminates the poor-performing ones while concentrating more resources on promising configurations.\n",
        "\n",
        "**Population-based Training (PBT)**: An approach that trains a population of models with different hyperparameters concurrently, and periodically adjusts these hyperparameters based on the performance of the models.\n",
        "\n",
        "In practice, the choice of hyperparameter tuning method depends on the size of the hyperparameter space, the computational resources available, the nature of the problem, and the time constraints. For deep learning, due to the larger computational cost and more extensive hyperparameter space, methods like Bayesian optimization, Hyperband, and PBT are often preferred, as they can be more efficient compared to grid or random search."
      ],
      "metadata": {
        "id": "WWUr-K_NvXq2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VkZHhvMsvRZy"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris\n",
        "X, y = iris.data, iris.target"
      ],
      "metadata": {
        "id": "327yUy0TvuAb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XJIM3WJpxNtj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Model\n",
        "svc = SVC()"
      ],
      "metadata": {
        "id": "tuasG5Ivy-zh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning the parameters\n",
        "\n",
        " In the context of support vector machines (SVMs), the parameters C, kernel, and gamma are critical hyperparameters that can significantly influence the performance of the model. When we perform hyperparameter tuning, we aim to find the best combination of these hyperparameters that will yield the highest accuracy (or another performance metric) for our model on the given data.\n",
        "\n",
        "Here's what each hyperparameter represents:\n",
        "\n",
        "C (Regularization parameter): The C parameter trades off correct classification of training examples against maximization of the decision function's margin. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors. C can be thought of as the \"cost\" of misclassification. A large C gives you low bias and high variance, while a small C will give you higher bias and lower variance.\n",
        "\n",
        "kernel: The kernel type to be used in the SVM algorithm. The kernel function transforms the input data into a higher dimensional space where a linear separator may be found if the data is not linearly separable in the original space. Common kernel functions include:\n",
        "\n",
        "linear: Linear kernel (no transformation).\n",
        "rbf: Radial basis function kernel (default), good for non-linear data.\n",
        "poly: Polynomial kernel.\n",
        "sigmoid: Sigmoid kernel.\n",
        "gamma (Kernel coefficient for 'rbf', 'poly', and 'sigmoid'): Defines how far the influence of a single training example reaches. Low values mean 'far' and high values mean 'close'. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
        "\n",
        "\n",
        "In this dictionary:\n",
        "\n",
        "C is set to be tested with the values [1, 10, 100, 1000].\n",
        "\n",
        "kernel is set to try all four listed types: ['linear', 'rbf', 'poly', 'sigmoid'].\n",
        "\n",
        "gamma is set to try 'scale' (uses 1 / (n_features * X.var()) as value of gamma), 'auto' (uses 1 / n_features), and the specific values [0.001, 0.0001].\n",
        "\n",
        "The GridSearchCV function from scikit-learn will then perform a grid search over all possible combinations of these hyperparameter values, which in this case would be 4 (C values) x 4 (kernel types) x 4 (gamma values) = 64 models to train and evaluate using cross-validation to find the best combination based on the selected performance metric."
      ],
      "metadata": {
        "id": "iuLGNTewrSbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = {\n",
        "    'C': [1, 10, 100, 1000],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.0001],\n",
        "}"
      ],
      "metadata": {
        "id": "yBd7nUibzEes"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate GridSearchCV object\n",
        "clf = GridSearchCV(svc, tuned_parameters, cv=5, scoring='accuracy')"
      ],
      "metadata": {
        "id": "SRcLCph24PaU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the grid search to the data\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "VKbtotpr4aSf",
        "outputId": "4c838fc3-9076-402c-989b-f7336bf190c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=SVC(),\n",
              "             param_grid={'C': [1, 10, 100, 1000],\n",
              "                         'gamma': ['scale', 'auto', 0.001, 0.0001],\n",
              "                         'kernel': ['linear', 'rbf', 'poly', 'sigmoid']},\n",
              "             scoring='accuracy')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
              "             param_grid={&#x27;C&#x27;: [1, 10, 100, 1000],\n",
              "                         &#x27;gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;, 0.001, 0.0001],\n",
              "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;, &#x27;poly&#x27;, &#x27;sigmoid&#x27;]},\n",
              "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
              "             param_grid={&#x27;C&#x27;: [1, 10, 100, 1000],\n",
              "                         &#x27;gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;, 0.001, 0.0001],\n",
              "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;, &#x27;poly&#x27;, &#x27;sigmoid&#x27;]},\n",
              "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found on training set: \")\n",
        "print(clf.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXBQq8aW4fV1",
        "outputId": "2cba7aa1-0092-460d-91f0-3bf6b6d936fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found on training set: \n",
            "{'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model found the test set\n",
        "print(\"Detailed Classification Report:\")\n",
        "y_true, y_pred = y_test, clf.predict(X_test)\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9NZZpTc4qPO",
        "outputId": "6918e5e1-0e79-4a93-89ab-28ca782f2450"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning Example in DL"
      ],
      "metadata": {
        "id": "Ow4E-LIfcKAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "X-8hwsXc4-Cx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8QcOhmycb6J",
        "outputId": "fd31a2d1-b4d5-40e2-aa33-0f64b40efc3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m92.2/128.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.11.17)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_tuner.tuners import RandomSearch"
      ],
      "metadata": {
        "id": "UY8s7k4edt74"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "x_train = x_train.reshape(-1, 28*28)\n",
        "x_test = x_test.reshape(-1, 28*28)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zO1w64Oh8Tx",
        "outputId": "a70ea4db-d885-4266-b33d-0bf95f13adbe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model builder function\n",
        "def build_model(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Input(shape=(784,)))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(layers.Dense(units=hp_units, activation='relu'))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "aM9UAvw_in07"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='my_dir',\n",
        "    project_name='helloworld'\n",
        ")"
      ],
      "metadata": {
        "id": "Mz4cKiMzi3tT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a callback to stop training early after reaching a certain value for the validation loss\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "metadata": {
        "id": "YLjE3yEWkOyc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gc import callbacks\n",
        "# Execute the hyperparameter search\n",
        "tuner.search(x_train, y_train, epochs=5, validation_split=0.2, callbacks=[stop_early])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AFsLS_mlEOA",
        "outputId": "c4d0e56e-f8fc-42c0-bebd-7e5e9e7b8261"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 03m 02s]\n",
            "val_accuracy: 0.9588333169619242\n",
            "\n",
            "Best val_accuracy So Far: 0.9763611157735189\n",
            "Total elapsed time: 00h 13m 42s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n"
      ],
      "metadata": {
        "id": "C3L3Kb3zlRz_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFhPKsvN0DiX",
        "outputId": "0c304635-742b-4dda-b55c-c400a6102fc8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
            "layer is 352 and the optimal learning rate for the optimizer\n",
            "is 0.001.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(x_train, y_train, epochs=5, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFZ30Kn9mSfV",
        "outputId": "3c4e1655-9183-4fb2-a3a7-2d736da014ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.2368 - accuracy: 0.9313 - val_loss: 0.1202 - val_accuracy: 0.9653\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0964 - accuracy: 0.9712 - val_loss: 0.1031 - val_accuracy: 0.9688\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0627 - accuracy: 0.9807 - val_loss: 0.0895 - val_accuracy: 0.9736\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0435 - accuracy: 0.9868 - val_loss: 0.0878 - val_accuracy: 0.9735\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0310 - accuracy: 0.9902 - val_loss: 0.0754 - val_accuracy: 0.9785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oo4y1G49uJhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}